{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from conf import settings\n",
    "from utils import get_network, get_training_dataloader, get_test_dataloader, WarmUpLR, \\\n",
    "    most_recent_folder, most_recent_weights, last_epoch, best_acc_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from models.vgg import vgg19_bn\n",
    "net0= vgg19_bn()\n",
    "net= \"vgg19\"\n",
    "gpu=False\n",
    "b=128\n",
    "warm=1\n",
    "lr=0.1\n",
    "resume=False\n",
    "#data preprocessing:\n",
    "cifar100_training_loader = get_training_dataloader(\n",
    "    settings.CIFAR100_TRAIN_MEAN,\n",
    "    settings.CIFAR100_TRAIN_STD,\n",
    "    num_workers=4,\n",
    "    batch_size=b,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "cifar100_test_loader = get_test_dataloader(\n",
    "    settings.CIFAR100_TRAIN_MEAN,\n",
    "    settings.CIFAR100_TRAIN_STD,\n",
    "    num_workers=4,\n",
    "    batch_size=b,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net0.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=settings.MILESTONES, gamma=0.2) #learning rate decay\n",
    "iter_per_epoch = len(cifar100_training_loader)\n",
    "warmup_scheduler = WarmUpLR(optimizer, iter_per_epoch * warm)\n",
    "writer = SummaryWriter(log_dir=os.path.join(\n",
    "            settings.LOG_DIR, net, settings.TIME_NOW))\n",
    "def train(epoch):\n",
    "\n",
    "    start = time.time()\n",
    "    net0.train()\n",
    "    for batch_index, (images, labels) in enumerate(cifar100_training_loader):\n",
    "\n",
    "        if gpu:\n",
    "            labels = labels.cuda()\n",
    "            images = images.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net0(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        n_iter = (epoch - 1) * len(cifar100_training_loader) + batch_index + 1\n",
    "\n",
    "        last_layer = list(net0.children())[-1]\n",
    "        for name, para in last_layer.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                writer.add_scalar('LastLayerGradients/grad_norm2_weights', para.grad.norm(), n_iter)\n",
    "            if 'bias' in name:\n",
    "                writer.add_scalar('LastLayerGradients/grad_norm2_bias', para.grad.norm(), n_iter)\n",
    "\n",
    "        print('Training Epoch: {epoch} [{trained_samples}/{total_samples}]\\tLoss: {:0.4f}\\tLR: {:0.6f}'.format(\n",
    "            loss.item(),\n",
    "            optimizer.param_groups[0]['lr'],\n",
    "            epoch=epoch,\n",
    "            trained_samples=batch_index * b + len(images),\n",
    "            total_samples=len(cifar100_training_loader.dataset)\n",
    "        ))\n",
    "\n",
    "        #update training loss for each iteration\n",
    "        writer.add_scalar('Train/loss', loss.item(), n_iter)\n",
    "\n",
    "        if epoch <= warm:\n",
    "            warmup_scheduler.step()\n",
    "\n",
    "    for name, param in net0.named_parameters():\n",
    "        layer, attr = os.path.splitext(name)\n",
    "        attr = attr[1:]\n",
    "        writer.add_histogram(\"{}/{}\".format(layer, attr), param, epoch)\n",
    "\n",
    "    finish = time.time()\n",
    "\n",
    "    print('epoch {} training time consumed: {:.2f}s'.format(epoch, finish - start))\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_training(epoch=0, tb=True):\n",
    "\n",
    "    start = time.time()\n",
    "    net0.eval()\n",
    "\n",
    "    test_loss = 0.0 # cost function error\n",
    "    correct = 0.0\n",
    "\n",
    "    for (images, labels) in cifar100_test_loader:\n",
    "\n",
    "        if gpu:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        outputs = net0(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += preds.eq(labels).sum()\n",
    "\n",
    "    finish = time.time()\n",
    "    if gpu:\n",
    "        print('GPU INFO.....')\n",
    "        print(torch.cuda.memory_summary(), end='')\n",
    "    print('Evaluating Network.....')\n",
    "    print('Test set: Epoch: {}, Average loss: {:.4f}, Accuracy: {:.4f}, Time consumed:{:.2f}s'.format(\n",
    "        epoch,\n",
    "        test_loss / len(cifar100_test_loader.dataset),\n",
    "        correct.float() / len(cifar100_test_loader.dataset),\n",
    "        finish - start\n",
    "    ))\n",
    "    print()\n",
    "\n",
    "    #add informations to tensorboard\n",
    "    if tb:\n",
    "        writer.add_scalar('Test/Average loss', test_loss / len(cifar100_test_loader.dataset), epoch)\n",
    "        writer.add_scalar('Test/Accuracy', correct.float() / len(cifar100_test_loader.dataset), epoch)\n",
    "\n",
    "    return correct.float() / len(cifar100_test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training Epoch: 1 [128/50000]\tLoss: 4.6157\tLR: 0.000000\n",
      "Training Epoch: 1 [256/50000]\tLoss: 4.6318\tLR: 0.000256\n",
      "Training Epoch: 1 [384/50000]\tLoss: 4.6165\tLR: 0.000512\n",
      "Training Epoch: 1 [512/50000]\tLoss: 4.6358\tLR: 0.000767\n",
      "Training Epoch: 1 [640/50000]\tLoss: 4.6553\tLR: 0.001023\n",
      "Training Epoch: 1 [768/50000]\tLoss: 4.6382\tLR: 0.001279\n",
      "Training Epoch: 1 [896/50000]\tLoss: 4.6354\tLR: 0.001535\n",
      "Training Epoch: 1 [1024/50000]\tLoss: 4.6401\tLR: 0.001790\n",
      "Training Epoch: 1 [1152/50000]\tLoss: 4.6367\tLR: 0.002046\n",
      "Training Epoch: 1 [1280/50000]\tLoss: 4.6267\tLR: 0.002302\n",
      "Training Epoch: 1 [1408/50000]\tLoss: 4.6241\tLR: 0.002558\n",
      "Training Epoch: 1 [1536/50000]\tLoss: 4.6263\tLR: 0.002813\n",
      "Training Epoch: 1 [1664/50000]\tLoss: 4.6198\tLR: 0.003069\n",
      "Training Epoch: 1 [1792/50000]\tLoss: 4.6111\tLR: 0.003325\n",
      "Training Epoch: 1 [1920/50000]\tLoss: 4.6626\tLR: 0.003581\n",
      "Training Epoch: 1 [2048/50000]\tLoss: 4.6186\tLR: 0.003836\n",
      "Training Epoch: 1 [2176/50000]\tLoss: 4.5473\tLR: 0.004092\n",
      "Training Epoch: 1 [2304/50000]\tLoss: 4.6341\tLR: 0.004348\n",
      "Training Epoch: 1 [2432/50000]\tLoss: 4.6940\tLR: 0.004604\n",
      "Training Epoch: 1 [2560/50000]\tLoss: 4.6000\tLR: 0.004859\n",
      "Training Epoch: 1 [2688/50000]\tLoss: 4.5969\tLR: 0.005115\n",
      "Training Epoch: 1 [2816/50000]\tLoss: 4.5859\tLR: 0.005371\n",
      "Training Epoch: 1 [2944/50000]\tLoss: 4.6584\tLR: 0.005627\n",
      "Training Epoch: 1 [3072/50000]\tLoss: 4.6016\tLR: 0.005882\n",
      "Training Epoch: 1 [3200/50000]\tLoss: 4.6021\tLR: 0.006138\n",
      "Training Epoch: 1 [3328/50000]\tLoss: 4.5720\tLR: 0.006394\n",
      "Training Epoch: 1 [3456/50000]\tLoss: 4.5821\tLR: 0.006650\n",
      "Training Epoch: 1 [3584/50000]\tLoss: 4.5575\tLR: 0.006905\n",
      "Training Epoch: 1 [3712/50000]\tLoss: 4.5615\tLR: 0.007161\n",
      "Training Epoch: 1 [3840/50000]\tLoss: 4.6450\tLR: 0.007417\n",
      "Training Epoch: 1 [3968/50000]\tLoss: 4.5993\tLR: 0.007673\n",
      "Training Epoch: 1 [4096/50000]\tLoss: 4.5073\tLR: 0.007928\n",
      "Training Epoch: 1 [4224/50000]\tLoss: 4.5950\tLR: 0.008184\n",
      "Training Epoch: 1 [4352/50000]\tLoss: 4.5665\tLR: 0.008440\n",
      "Training Epoch: 1 [4480/50000]\tLoss: 4.5591\tLR: 0.008696\n",
      "Training Epoch: 1 [4608/50000]\tLoss: 4.5419\tLR: 0.008951\n",
      "Training Epoch: 1 [4736/50000]\tLoss: 4.6145\tLR: 0.009207\n",
      "Training Epoch: 1 [4864/50000]\tLoss: 4.6842\tLR: 0.009463\n",
      "Training Epoch: 1 [4992/50000]\tLoss: 4.7027\tLR: 0.009719\n",
      "Training Epoch: 1 [5120/50000]\tLoss: 4.6145\tLR: 0.009974\n",
      "Training Epoch: 1 [5248/50000]\tLoss: 4.6540\tLR: 0.010230\n",
      "Training Epoch: 1 [5376/50000]\tLoss: 4.5943\tLR: 0.010486\n",
      "Training Epoch: 1 [5504/50000]\tLoss: 4.5583\tLR: 0.010742\n",
      "Training Epoch: 1 [5632/50000]\tLoss: 4.5586\tLR: 0.010997\n",
      "Training Epoch: 1 [5760/50000]\tLoss: 4.6806\tLR: 0.011253\n",
      "Training Epoch: 1 [5888/50000]\tLoss: 4.5941\tLR: 0.011509\n",
      "Training Epoch: 1 [6016/50000]\tLoss: 4.5666\tLR: 0.011765\n",
      "Training Epoch: 1 [6144/50000]\tLoss: 4.5933\tLR: 0.012020\n",
      "Training Epoch: 1 [6272/50000]\tLoss: 4.5361\tLR: 0.012276\n",
      "Training Epoch: 1 [6400/50000]\tLoss: 4.5939\tLR: 0.012532\n",
      "Training Epoch: 1 [6528/50000]\tLoss: 4.5011\tLR: 0.012788\n",
      "Training Epoch: 1 [6656/50000]\tLoss: 4.6018\tLR: 0.013043\n",
      "Training Epoch: 1 [6784/50000]\tLoss: 4.6530\tLR: 0.013299\n",
      "Training Epoch: 1 [6912/50000]\tLoss: 4.6273\tLR: 0.013555\n",
      "Training Epoch: 1 [7040/50000]\tLoss: 4.4773\tLR: 0.013811\n",
      "Training Epoch: 1 [7168/50000]\tLoss: 4.5718\tLR: 0.014066\n",
      "Training Epoch: 1 [7296/50000]\tLoss: 4.5271\tLR: 0.014322\n",
      "Training Epoch: 1 [7424/50000]\tLoss: 4.5723\tLR: 0.014578\n",
      "Training Epoch: 1 [7552/50000]\tLoss: 4.5278\tLR: 0.014834\n",
      "Training Epoch: 1 [7680/50000]\tLoss: 4.5074\tLR: 0.015090\n",
      "Training Epoch: 1 [7808/50000]\tLoss: 4.5342\tLR: 0.015345\n",
      "Training Epoch: 1 [7936/50000]\tLoss: 4.4729\tLR: 0.015601\n",
      "Training Epoch: 1 [8064/50000]\tLoss: 4.5007\tLR: 0.015857\n",
      "Training Epoch: 1 [8192/50000]\tLoss: 4.5681\tLR: 0.016113\n",
      "Training Epoch: 1 [8320/50000]\tLoss: 4.5014\tLR: 0.016368\n",
      "Training Epoch: 1 [8448/50000]\tLoss: 4.5670\tLR: 0.016624\n",
      "Training Epoch: 1 [8576/50000]\tLoss: 4.4342\tLR: 0.016880\n",
      "Training Epoch: 1 [8704/50000]\tLoss: 4.5785\tLR: 0.017136\n",
      "Training Epoch: 1 [8832/50000]\tLoss: 4.4897\tLR: 0.017391\n",
      "Training Epoch: 1 [8960/50000]\tLoss: 4.5507\tLR: 0.017647\n",
      "Training Epoch: 1 [9088/50000]\tLoss: 4.6322\tLR: 0.017903\n",
      "Training Epoch: 1 [9216/50000]\tLoss: 4.4984\tLR: 0.018159\n",
      "Training Epoch: 1 [9344/50000]\tLoss: 4.5221\tLR: 0.018414\n",
      "Training Epoch: 1 [9472/50000]\tLoss: 4.5813\tLR: 0.018670\n",
      "Training Epoch: 1 [9600/50000]\tLoss: 4.5118\tLR: 0.018926\n",
      "Training Epoch: 1 [9728/50000]\tLoss: 4.5398\tLR: 0.019182\n",
      "Training Epoch: 1 [9856/50000]\tLoss: 4.4652\tLR: 0.019437\n",
      "Training Epoch: 1 [9984/50000]\tLoss: 4.5191\tLR: 0.019693\n",
      "Training Epoch: 1 [10112/50000]\tLoss: 4.5807\tLR: 0.019949\n",
      "Training Epoch: 1 [10240/50000]\tLoss: 4.4192\tLR: 0.020205\n",
      "Training Epoch: 1 [10368/50000]\tLoss: 4.4653\tLR: 0.020460\n",
      "Training Epoch: 1 [10496/50000]\tLoss: 4.4467\tLR: 0.020716\n",
      "Training Epoch: 1 [10624/50000]\tLoss: 4.3707\tLR: 0.020972\n",
      "Training Epoch: 1 [10752/50000]\tLoss: 4.4962\tLR: 0.021228\n",
      "Training Epoch: 1 [10880/50000]\tLoss: 4.4079\tLR: 0.021483\n",
      "Training Epoch: 1 [11008/50000]\tLoss: 4.5786\tLR: 0.021739\n",
      "Training Epoch: 1 [11136/50000]\tLoss: 4.3674\tLR: 0.021995\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f8742a8fe8e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-24bce22f40a0>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(description='models for classification in cifar100 Training With Pytorch')\n",
    "    parser.add_argument('--net', type=str,  default=\"mobilenet\", help='net type')  \n",
    "    #default=\"vgg19\"  default=\"densenet121\" default=\"resnet101\"  default=\"shufflenet\" default=\"seresnet101\"  default=\"mobilenet\"\n",
    "    parser.add_argument('--gpu', action='store_true', default=False, help='use gpu or not')\n",
    "    parser.add_argument('--b', type=int, default=128, help='batch size for dataloader')\n",
    "    parser.add_argument('--warm', type=int, default=1, help='warm up training phase')\n",
    "    parser.add_argument('--lr', type=float, default=0.1, help='initial learning rate')\n",
    "    parser.add_argument('--resume', action='store_true', default=False, help='resume training')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    net = get_network(args)\n",
    "    '''\n",
    "    from models.vgg import vgg19_bn\n",
    "    net0= vgg19_bn()\n",
    "    net= \"vgg19\"\n",
    "    gpu=False\n",
    "    b=128\n",
    "    warm=1\n",
    "    lr=0.1\n",
    "    resume=False\n",
    "    #data preprocessing:\n",
    "    cifar100_training_loader = get_training_dataloader(\n",
    "        settings.CIFAR100_TRAIN_MEAN,\n",
    "        settings.CIFAR100_TRAIN_STD,\n",
    "        num_workers=4,\n",
    "        batch_size=b,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    cifar100_test_loader = get_test_dataloader(\n",
    "        settings.CIFAR100_TRAIN_MEAN,\n",
    "        settings.CIFAR100_TRAIN_STD,\n",
    "        num_workers=4,\n",
    "        batch_size=b,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net0.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=settings.MILESTONES, gamma=0.2) #learning rate decay\n",
    "    iter_per_epoch = len(cifar100_training_loader)\n",
    "    warmup_scheduler = WarmUpLR(optimizer, iter_per_epoch * warm)\n",
    "\n",
    "    if resume:\n",
    "        recent_folder = most_recent_folder(os.path.join(settings.CHECKPOINT_PATH, net), fmt=settings.DATE_FORMAT)\n",
    "        if not recent_folder:\n",
    "            raise Exception('no recent folder were found')\n",
    "\n",
    "        checkpoint_path = os.path.join(settings.CHECKPOINT_PATH, net, recent_folder)\n",
    "\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(settings.CHECKPOINT_PATH, net, settings.TIME_NOW)\n",
    "\n",
    "    #use tensorboard\n",
    "    if not os.path.exists(settings.LOG_DIR):\n",
    "        os.mkdir(settings.LOG_DIR)\n",
    "\n",
    "    #since tensorboard can't overwrite old values\n",
    "    #so the only way is to create a new tensorboard log\n",
    "    writer = SummaryWriter(log_dir=os.path.join(\n",
    "            settings.LOG_DIR, net, settings.TIME_NOW))\n",
    "    input_tensor = torch.Tensor(1, 3, 32, 32)\n",
    "    if gpu:\n",
    "        input_tensor = input_tensor.cuda()\n",
    "    writer.add_graph(net0, input_tensor)\n",
    "\n",
    "    #create checkpoint folder to save model\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        os.makedirs(checkpoint_path)\n",
    "    checkpoint_path = os.path.join(checkpoint_path, '{net}-{epoch}-{type}.pth')\n",
    "\n",
    "    best_acc = 0.0\n",
    "    if resume:\n",
    "        best_weights = best_acc_weights(os.path.join(settings.CHECKPOINT_PATH, args.net, recent_folder))\n",
    "        if best_weights:\n",
    "            weights_path = os.path.join(settings.CHECKPOINT_PATH, net, recent_folder, best_weights)\n",
    "            print('found best acc weights file:{}'.format(weights_path))\n",
    "            print('load best training file to test acc...')\n",
    "            net.load_state_dict(torch.load(weights_path))\n",
    "            best_acc = eval_training(tb=False)\n",
    "            print('best acc is {:0.2f}'.format(best_acc))\n",
    "\n",
    "        recent_weights_file = most_recent_weights(os.path.join(settings.CHECKPOINT_PATH, net, recent_folder))\n",
    "        if not recent_weights_file:\n",
    "            raise Exception('no recent weights file were found')\n",
    "        weights_path = os.path.join(settings.CHECKPOINT_PATH, net, recent_folder, recent_weights_file)\n",
    "        print('loading weights file {} to resume training.....'.format(weights_path))\n",
    "        net.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "        resume_epoch = last_epoch(os.path.join(settings.CHECKPOINT_PATH, net, recent_folder))\n",
    "\n",
    "\n",
    "    for epoch in range(1, settings.EPOCH + 1):\n",
    "        if epoch > warm:\n",
    "            train_scheduler.step(epoch)\n",
    "\n",
    "        if resume:\n",
    "            if epoch <= resume_epoch:\n",
    "                continue\n",
    "\n",
    "        train(epoch)\n",
    "        acc = eval_training(epoch)\n",
    "\n",
    "        #start to save best performance model after learning rate decay to 0.01\n",
    "        if epoch > settings.MILESTONES[1] and best_acc < acc:\n",
    "            weights_path = checkpoint_path.format(net=net, epoch=epoch, type='best')\n",
    "            print('saving weights file to {}'.format(weights_path))\n",
    "            torch.save(net.state_dict(), weights_path)\n",
    "            best_acc = acc\n",
    "            continue\n",
    "\n",
    "        if not epoch % settings.SAVE_EPOCH:\n",
    "            weights_path = checkpoint_path.format(net=net, epoch=epoch, type='regular')\n",
    "            print('saving weights file to {}'.format(weights_path))\n",
    "            torch.save(net.state_dict(), weights_path)\n",
    "\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
